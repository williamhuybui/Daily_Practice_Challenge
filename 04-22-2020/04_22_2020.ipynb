{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General\n",
    "\n",
    "Goal: Reduce the dimension to simplify the problem and speed up the machine learning model.\n",
    "\n",
    "### Term:\n",
    "\n",
    "**The Curse of Dimensionality**: weird things happen in higher-dimensional space. For example: \n",
    "\n",
    "1) The probability of choosing a point in a unit square that is 0.001 away from the border is 0.4%. But for a 10000-dimensional unit hypercube, this probability would be 99.999999%\n",
    "\n",
    "2) The average length of 2 random points in a unit square are 0.52, but in the 1000000-dim hypercube is 408.25\n",
    "\n",
    "We will verify these facts with python later. \n",
    "\n",
    "**Manifold**: a d-dimensional manifold is a part of an n-dimensional space (d<n) that locally resembles a d-dimensional hyperplane. For example: loosely speaking, a leaf is a 2d-manifold in a 3d-world since its local resemble a 2D plane.\n",
    "\n",
    "**Manifold hypothesis/assumption**: high dimensional real-world datasets lie close to a much lower-dimensional manifold. The way to select this manifold is different for each data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of random points in a square: 0.004004004004004004\n",
      "Probability of random points in a 10000-dim hypercube: 1.0\n"
     ]
    }
   ],
   "source": [
    "#Curse of dimensionality illustration\n",
    "\n",
    "#1) Choosing a poin in hypercube\n",
    "import random \n",
    "random.seed(2)\n",
    "\n",
    "num_sample=999\n",
    "num_dim=10000\n",
    "\n",
    "#1000 points in a unit square\n",
    "square_pts=np.random.uniform(0,1,size=(num_sample,2))\n",
    "#1000 points in of unit hypercube\n",
    "hypercube=np.random.uniform(0,1,size=(num_sample,num_dim))\n",
    "\n",
    "#Function that count howmany point is in the \"restricted area\" away the border\n",
    "def closed2border(points,distance_border):\n",
    "    \n",
    "    #Thought: if one of the coordinate is within the range, then the point is within the restricted area\n",
    "    count=0\n",
    "    for point in points:\n",
    "        for cord in point:\n",
    "            if cord > (1-distance_border):\n",
    "                count+=1\n",
    "                break\n",
    "    return count\n",
    "\n",
    "print(\"Probability of random points in a square:\", closed2border(square_pts,0.001)/num_sample)\n",
    "print(\"Probability of random points in a {}-dim hypercube:\".format(num_dim), \n",
    "                                  closed2border(hypercube,0.001)/num_sample)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average distance in a square: 0.5155844529601187\n",
      "Average distance in a 100000-dimensional hypercube: 129.10576760497972\n"
     ]
    }
   ],
   "source": [
    "# 2) The average length of 2 random points in a unit square are 0.52, but in the 1000000-dim hypercube is 408.25\n",
    "\n",
    "from scipy.spatial import distance\n",
    "num_sample=999\n",
    "num_dim=100000 #I reduced the size cause 1 million dim takes too long\n",
    "\n",
    "#1000 points in a unit square\n",
    "square_pts=np.random.uniform(0,1,size=(num_sample,2))\n",
    "#1000 points in of unit hypercube\n",
    "hypercube=np.random.uniform(0,1,size=(num_sample,num_dim))\n",
    "\n",
    "\n",
    "def average_distance(points):\n",
    "    half_len=int(len(points)/2)\n",
    "    pairs=list(zip(points[:half_len+1],points[half_len:2*half_len+2]))\n",
    "    mean_dist = np.mean([distance.euclidean(p1, p2) for p1, p2 in pairs])\n",
    "    return mean_dist\n",
    "\n",
    "print(\"Average distance in a square:\", average_distance(square_pts))\n",
    "print(\"Average distance in a {}-dimensional hypercube:\".format(num_dim), average_distance(hypercube))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "Idea: **Detect the best hyperplane for the data and project the data on our hyperplane**\n",
    "\n",
    "The \"best\" hyperplane is the one that can preserve the variances of the data. In other words, this plane can capture the most characteristics of the data. \n",
    "\n",
    "Note that, a plane can be broken down into **principal components** which are unit vector that is orthogonal to each other. There are many ways to choose  **principal components** but we want to choose those that preserve the most variances of the data on its axis. One way to achieve it is to find the **eigenvectors of the covariance matrix**. The explaination for this will be in another paper. The values of the corresponding eigenvalue of each eigenvector will tell how much variances each vector preserves. Finally, we can combine these vector to form a matrix transformation that project data points to a new hyperplane. \n",
    "\n",
    "Assume that our data has d-dimension, (not counting the label). There are 3 steps in total to achieve our goal:\n",
    "\n",
    "\n",
    "1) Compute the covariance matrix of the whole dataset.\n",
    "\n",
    "2) Compute eigenvectors and the corresponding eigenvalues\n",
    "\n",
    "3) Projection onto the new subspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Compute the covariance matrix of the whole dataset\n",
    "\n",
    "Assuming we have n features $\\left\\{x_{1}, \\ldots, x_{n}\\right\\}$, the covariance matrix $A$ is defined so that \n",
    "\n",
    "$$A_{i, j}=\\operatorname{Cov}\\left(x_{i}, x_{j}\\right)=\\frac{1}{n-1} \\sum_{i=1}^{n}(x_i-\\bar{x_i})(x_j-\\bar{x_j}) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Math</th>\n",
       "      <th>English</th>\n",
       "      <th>Art</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>60</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Math  English  Art\n",
       "0    90       60   90\n",
       "1    90       90   30\n",
       "2    60       60   60\n",
       "3    60       60   90\n",
       "4    30       30   30"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample\n",
    "import pandas as pd\n",
    "df=pd.DataFrame({\"Math\":[90,90,60,60,30],\"English\":[60,90,60,60,30],\"Art\":[90,30,60,90,30]})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Math</th>\n",
       "      <th>English</th>\n",
       "      <th>Art</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Math</th>\n",
       "      <td>504.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>360.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Art</th>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>720.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Math  English    Art\n",
       "Math     504.0    360.0  180.0\n",
       "English  360.0    360.0    0.0\n",
       "Art      180.0      0.0  720.0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Cov_entry(xi,xj):\n",
    "    meani=xi.mean()\n",
    "    meanj=xj.mean()\n",
    "    sum_=sum([(xi[index]-meani)*(xj[index]-meanj) for index in range(len(xi))])\n",
    "    return sum_/5\n",
    "\n",
    "def Cov(df):\n",
    "    columns=df.columns\n",
    "    matrix=[]\n",
    "    for i in columns:\n",
    "        row=[]\n",
    "        for j in columns:\n",
    "            row.append(Cov_entry(df[i],df[j]))\n",
    "        matrix.append(row)\n",
    "    df_new=pd.DataFrame(matrix,columns=columns, index=columns)\n",
    "    return df_new\n",
    "\n",
    "Cov(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance tell the relationship between feature $i,j$. For example, if a student is good at math, then they are more likely better at English than Art."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Compute eigenvectors and the corresponding eigenvalues\n",
    "\n",
    "An eigenvector is a vector $v$ whose direction remains unchanged when a linear transformation is applied to it, ie:\n",
    "\n",
    "$$ A v=\\lambda v $$\n",
    "\n",
    "Note that computing eigenvector requires another numerical approximation to find root of the **characteristic polynomials**. It is kind of cumbersome to create all the necessary tools to do the task. Hence, I will use numpy. The `np.linalg.eig` function might return a different eigenvector compare to when compute it by hand (Eigenvector is not unique). However, the eigenvalues are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigen vectors:\n",
      "[[ 0.6487899  -0.65580225 -0.3859988 ]\n",
      " [-0.74104991 -0.4291978  -0.51636642]\n",
      " [-0.17296443 -0.62105769  0.7644414 ]]\n",
      "Eigen values:\n",
      "[ 44.81966028 910.06995304 629.11038668]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def eig_val_vec(cov_df):\n",
    "    eig_val, eig_vec = np.linalg.eig(cov_df)\n",
    "    return eig_val, eig_vec\n",
    "\n",
    "eig_val, eig_vec=eig_val_vec(Cov(df))\n",
    "print(\"Eigen vectors:\")\n",
    "print(eig_vec)\n",
    "print(\"Eigen values:\")\n",
    "print(eig_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Projection onto the new subspace\n",
    "\n",
    "Let say we want to project the original data with dimention $d\\times n$ to a lower dimensional space $d\\times k$ (where $k <n$). There are 2 step\n",
    "\n",
    "* Sort the eigenvectors by decreasing eigenvalues and choose $k$ eigenvectors with the largest eigenvalues to form a $d × k$ dimensional matrix $W$.\n",
    "\n",
    "* Use this $d × k$ eigenvector matrix to transform the samples onto the new subspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finalize\n",
    "def projection(df,k):\n",
    "    #Get the eigen values and vectors of the covariance matrix of df\n",
    "    eig_val, eig_vec=eig_val_vec(Cov(df)) \n",
    "    \n",
    "    #Sort them \n",
    "    list_=list(zip(eig_val,eig_vec))\n",
    "    list_.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    #Find the first k-eigenvector\n",
    "    eig_vec_matrix=[i[1] for i in list_[:k]]\n",
    "    eig_vec_matrix=np.array(eig_vec_matrix)\n",
    "    \n",
    "    #Transform\n",
    "    new_data=eig_vec_matrix.dot(df.values.T)\n",
    "    new_data=pd.DataFrame(new_data.T,columns=np.arange(len(new_data.T[0])))\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-138.919338</td>\n",
       "      <td>15.969466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-120.813286</td>\n",
       "      <td>-48.528749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-101.196848</td>\n",
       "      <td>-1.774843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-116.687840</td>\n",
       "      <td>21.158399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-50.598424</td>\n",
       "      <td>-0.887422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          1\n",
       "0 -138.919338  15.969466\n",
       "1 -120.813286 -48.528749\n",
       "2 -101.196848  -1.774843\n",
       "3 -116.687840  21.158399\n",
       "4  -50.598424  -0.887422"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projection(df,2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
